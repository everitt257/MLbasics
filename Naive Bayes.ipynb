{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML 2 Generative Learning\n",
    "In this notebook we make distinction between discriminative learning and generative learning algorithms.\n",
    "## Bayes Rule\n",
    "The bayes rule is used to derive the posterior distribution on y given x\n",
    "$$p(y\\mid x) = \\frac{p(x\\mid y)p(y)}{p(x)}$$\n",
    "\n",
    "Note we don't actually need to compute p(x) in order to make prediction. The prediction can be made sole based on:\n",
    "$$\n",
    "\\begin{split}\n",
    "\\arg\\max_{y}p(y\\mid x) &= \\arg\\max_{y}\\frac{p(x\\mid y)p(y)}{p(x)} \\\\\n",
    "&= \\arg\\max_{y}p(x\\mid y)p(y)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Given $p(y)$ the **class priors**. The features $x$ can be broken down into $x1, x2, x3 ... x_n$. This means to calculate $p(x\\mid y)$ we can rewrite as $p(x_1,x_2,x_3...x_n\\mid y)$. If we assume the features are indepently distributed, we can rewrite $p(x|y)$ as\n",
    "$$p(x_1,x_2,x_3...x_n\\mid y) = p(x_1\\mid y)p(x_2\\mid y)p(x_3\\mid y)...p(x_n\\mid y)$$\n",
    "\n",
    "All we need to do is selecting very good features to predict. Also I like to point out these indivisual distributions can be guessed or learned. Another thing is that even if these feautures are not indepent, the equation still holds for most of the time.\n",
    "\n",
    "## Gaussian discriminant analysis\n",
    "Before the derivation, we need to understand that what the term **probability of data** is.\n",
    ">The probability of the data is given by\n",
    "$p(\\vec{y}\\mid X; \\theta)$. This quantity is typically viewed a function of $\\vec{y}$ (and perhaps $X$),\n",
    "for a fixed value of $\\theta$\n",
    "\n",
    "- For least square problems, we are maximizing\n",
    "$$\n",
    "\\ell(w) = \\prod_{i=1}^{m}{p(y^i|x^i;w)} = \\prod_{i=1}^{m}{\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(y^i-w^Tx^i)^2}{2\\sigma^2}}}\n",
    "$$\n",
    "\n",
    "- For logistic regression, we are maximizing\n",
    "$$\n",
    "\\ell(\\theta) = \\prod_{i=1}^{m}{p(y^i|x^i;\\theta)} = \\prod_{i=1}^{m}{(h_{\\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{1-y^{(i)}}}\n",
    "$$\n",
    "    - We then take the log of this function since it make the life easier.\n",
    "\n",
    "- For naive bayes of binary output, we are actually maximizing the joint distribution of data, since the our probability distribution depends on both features and labels.\n",
    "\n",
    "\\begin{gather*}\n",
    "\\ell(\\theta,\\mu_{0},\\mu_{1},\\Sigma) = log\\prod_{i=1}^{m}{p(x^i,y^i;\\theta,\\mu_0,\\mu_1,\\Sigma)} \\\\\n",
    "y\\sim{Bernoulli(\\theta)} \\\\\n",
    "x\\mid y=0 \\sim \\mathcal{N}(\\mu_{0},\\,\\Sigma) \\\\\n",
    "x\\mid y=1 \\sim \\mathcal{N}(\\mu_{1},\\,\\Sigma)\n",
    "\\end{gather*}\n",
    "\n",
    "### Derivation of Naive Bayes\n",
    "I'll skip this part for now\n",
    "\n",
    "### Variations of Naive Bayes\n",
    "- Multi-variate Bernoulli\n",
    "    - Good for binary feature vector\n",
    "    - Good for modeling text classification, word vector model. [0, 1, 0, ...]\n",
    "- Multinomial Model\n",
    "    - Quite similiar to Bernoulli Naive bayese, but the distribution is multinomial. For large training set, the accuracy is usually better than the previous one. However this is from empirical experience, in fact the feature selection part largely determines the performance.\n",
    "    - Good for modeling text classification as well, word event model.\n",
    "- LDA\n",
    "    - Continous features, covarience matrix doesn't change\n",
    "- QDA\n",
    "    - Continous features, covarience matrix change depending on prior classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
