{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List for research in deep learning\n",
    "1. Activation function\n",
    "2. Loss function\n",
    "3. Follow Standford CS 229\n",
    "     - This is crucial since we are trying to understand the fundementals of ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:(400, 13)        \n",
      "Weight shape: (13, 1)        \n",
      "Ground_Truth shape: (400, 1)        \n",
      "Bias 0.3385973288082881\n"
     ]
    }
   ],
   "source": [
    "# Part 1 Linear Regression\n",
    "housing_data = np.loadtxt(\"/home/everitt257/Downloads/stanford_dl_ex-master/ex1/housing.data\")\n",
    "features = housing_data[:400, :-1]\n",
    "ground_truths = housing_data[:400, -1].reshape(400,-1)\n",
    "# Number of parameters to be trained\n",
    "n = features.shape[1] # Number of columns\n",
    "m = features.shape[0] # Number of rows\n",
    "# Initialize the weights and bias\n",
    "weights = np.random.random_sample(n).reshape(n,-1)\n",
    "bias = np.random.random_sample()\n",
    "# Visualize what the matrix look like\n",
    "print(\"Feature shape:{} \\\n",
    "       \\nWeight shape: {} \\\n",
    "       \\nGround_Truth shape: {} \\\n",
    "       \\nBias {}\".format(features.shape, weights.shape, ground_truths.shape,bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the line\n",
    "$$ \\hat{y_i} = wx^i = w_0x_0^i + w_1x_1^i + ... + w_nx_n^i + b $$\n",
    "The $\\hat{y}$ should be as close to the original label $y$ as possible. Here I specify the loss function as:\n",
    "$$loss = \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_i} - y_i)^2$$\n",
    "What we are trying to do here is to minimize the loss function. To minmize the loss function. We'll use the \n",
    "gradient decent rule to help us decide what the partial derivatives are for each weight and bias.\n",
    "**Please note that we can have other loss function defined as well. The reason I choose $loss = \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_i} - y_i)^2$ is because it penalizes error quadratically. Secondly it make the math easier when we take the partial derivative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's compute some derivatives\n",
    "    \n",
    "    According gradient decent \n",
    "    \n",
    "    $$\\hat{w_i} = w_i - \\alpha\\frac{\\partial loss}{\\partial w}$$\n",
    "    \n",
    "    Where $w$ is the weight or bias. *To simplify things, we omits bias for the moment.*\n",
    "    In our case of study, let's **work out the partial derivative for a single feature**:\n",
    "    \n",
    "    $$loss_{single} = \\frac{1}{2}(\\hat{y_i} - y_i)^2$$\n",
    "    \n",
    "    The derived derivative w.r.t to weight is thus just:\n",
    "    \n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "        \\frac{\\partial loss}{\\partial w_i} & =  \\frac{\\partial loss}{\\partial \\hat{y_i}}\n",
    "        \\frac{\\partial \\hat{y_i}}{w_i} \\\\\n",
    "        & = (\\hat{y_i} - y_i)x_i\n",
    "        \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "    The derived derivative w.r.t to bias is\n",
    "    \n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "        \\frac{\\partial loss}{\\partial b} & =  \\frac{\\partial loss}{\\partial \\hat{y_i}}\n",
    "        \\frac{\\partial \\hat{y_i}}{b} \\\\\n",
    "            & = (\\hat{y_i} - y_i)\n",
    "        \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "    So the update rule for the weight and bias of  single feature is just \n",
    "    \n",
    "    \\begin{align}\n",
    "        \\hat{w_i} & = w_i + \\alpha(y_i-\\hat{y_i})x_i \\\\\n",
    "        \\hat{b} & = b + \\alpha(y_i-\\hat{y_i})\n",
    "    \\end{align}\n",
    "    \n",
    "    For multiple features, the rule can be generalized as:\n",
    "    \n",
    "    \\begin{align}\n",
    "        \\hat{w_i} & = w_i + \\alpha\\sum_{j=1}^{n}(y_i - \\hat{y_i})x_i^j \\\\\n",
    "        \\hat{b} & = b + \\alpha\\sum_{j=1}^{n}(y_i-\\hat{y_i})\n",
    "    \\end{align}\n",
    "    \n",
    "    Where $\\hat{y_i} = wx^i$ and $\\alpha$ equals the learning rate.\n",
    "    \n",
    "    **Note this is the gradient decent rule that follows the steepest decent. In practice we rarely consider this since it takes every derivative into account. Another solution is to use the so-called stochastic decent to accelerate the optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(weights):\n",
    "    \"\"\"Don't think I will be needing thi\"\"\"\n",
    "    return weights/np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_d(features, weights, labels):\n",
    "    y_hat = np.dot(features, weights)\n",
    "    y_diff = y_hat - labels\n",
    "    weight_d = np.dot(y_diff.transpose(), features)\n",
    "    bias_d = np.sum(y_diff)\n",
    "    return weight_d, bias_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_d, bias_d = partial_d(features, weights, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(old_w, old_b, weight_d, bias_d, a):\n",
    "    new_w = old_w + weight_d*a\n",
    "    new_b = old_b + bias_d*a\n",
    "    return new_w, new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w, new_b = update(weights, bias, weight_d, bias_d, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features.shape (400, 13) and Weights.shape (13, 13)\n",
      "(400, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features.shape {} and Weights.shape {}\".format(features.shape, weights.shape))\n",
    "y_hat = np.dot(features, weights)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n",
      "inf\n",
      "square_error is: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/everitt257/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in square\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "learn_rate = 0.001\n",
    "error_bound = 0.01\n",
    "for i in range(10):\n",
    "    weight_d, bias_d = partial_d(features, weights, ground_truths)\n",
    "    weights, bias = update(weights, bias, weight_d, bias_d, learn_rate)\n",
    "    \n",
    "    y_hat = np.dot(features, weights.transpose())\n",
    "    square_error = 1./400*np.sum((y_hat-ground_truths)**2)\n",
    "    print(square_error)\n",
    "    print(\"square_error is: \".format(square_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.random_sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.reshape(10,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1119970072086911"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_w = normalize(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06432218],\n",
       "       [ 0.14666415],\n",
       "       [ 0.04983606],\n",
       "       [ 0.01719232],\n",
       "       [ 0.09797791],\n",
       "       [ 0.04185312],\n",
       "       [ 0.11315203],\n",
       "       [ 0.11876532],\n",
       "       [ 0.08418966],\n",
       "       [ 0.02731797],\n",
       "       [ 0.02032364],\n",
       "       [ 0.08274421],\n",
       "       [ 0.13566143]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
