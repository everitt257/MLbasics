{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning theory\n",
    "### Generalization error\n",
    "**Questions** that we might want to ask:\n",
    "- Most learning algorithms fit their models to the training set, why should doing well on the training set tell us anything about generalization error?\n",
    "- Can we relate error on the training set to generalization error?\n",
    "- Are there conditions under which we can actually prove that learning algorithms will work well?\n",
    "\n",
    "**Hoeffding inequality** (Perhaps the most important inequality in learning theory)\n",
    "1. Markov's inequality:\n",
    "$$\\operatorname{\\mathbb{P}}(Z\\geq t) \\leq \\frac{\\operatorname{\\mathbb{E}}[Z]}{t}$$\n",
    "    * Link to the [proof](http://cs229.stanford.edu/extra-notes/hoeffding.pdf).\n",
    "\n",
    "2. Chebyshev's inequality: A consequence of Markov's inequality\n",
    "$$\n",
    "\\begin{split}\n",
    "\\operatorname{\\mathbb{P}}(Z \\geq \\operatorname{\\mathbb{E}}+t\\; or \\;Z \\leq \\operatorname{\\mathbb{E}(Z)}-t) &= \\operatorname{\\mathbb{P}}((Z - \\operatorname{\\mathbb{E}(Z)})^2 \\geq t^2) \\\\\n",
    "& \\leq \\frac{\\operatorname{\\mathbb{E}}[(Z-\\operatorname{\\mathbb{E}}(Z))^2]}{t^2} = \\frac{Var(Z)}{t^2}\n",
    "\\end{split}\n",
    "$$\n",
    "    * What this means is that average of random variables with finite variance converges to their mean. Given enough samples.\n",
    "3. Chernoff bounds\n",
    "    * Essentially it's saying, for any $\\lambda$, we have $Z \\geq \\operatorname{\\mathbb{E}}[Z] + t$ if and only if $e^{\\lambda Z} \\geq e^{\\lambda \\operatorname{\\mathbb{E}}[Z] + \\lambda t}$ or $e^{\\lambda (Z-\\operatorname{\\mathbb{E}}[Z]) \\geq e^{\\lambda t}}$\n",
    "    * Chernoff bounds use moment generating functions in a way to give exponential deviation bounds.\n",
    "    * The Chernoff bounds proof is done based on the combination of the moment function of random variable and the markov inequality. \n",
    "    * It also works well with r.v that is the sum of i.i.d random variables. For example it gives bounds in exponential form for r.v that is the sum of i.i.d random variables. \n",
    "4. Jensen's inequality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
