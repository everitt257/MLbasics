{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "### The bias-variance tradeoff -- linear regression case\n",
    "In machine learning we often hear the term \"overfitting\" and \"underfitting\". But what do they really meant. In this section we briefly explain these concept through math induction.\n",
    "Assume we want to come up with a machine learning model $\\hat{f}(x)$ that maps each data $x$ in my data set to some label $y$. Then a natural solution to measure the performance of our model is to apply the mean squared error(MSE).\n",
    "\n",
    "$$MSE = \\operatorname{\\mathbb{E}_{(x,y)}}\\lvert \\hat{f}(x) - y \\rvert^2$$\n",
    "\n",
    "**Our intuition**\n",
    "- Overfitting: the model doesn't generalize well to other dataset\n",
    "- Underfitting: the model wasn't well trained and the predicted data seems well off from the target y\n",
    "- Other reasons: the model's environment is noisy, or that the dataset is noisy\n",
    "\n",
    "**Intuition formalized:**\n",
    "The model can be seen as:\n",
    "$$y_i = f(x_i) + \\epsilon_i$$\n",
    "where the noise satifies $\\operatorname{\\mathbb{E}}(\\epsilon_i) = 0$ and $Var(\\epsilon_i) = \\sigma^2$.\n",
    "\n",
    "The key obervation here is that $\\hat{f}(x)$ is a random variable since it depends on the error term. The error term by itself is a random variable thus $\\hat{f}(x)$ is r.v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "MSE &= \\operatorname{\\mathbb{E}}(y - \\hat{f}(x))^2 \\\\\n",
    "    &= \\operatorname{\\mathbb{E}}((\\epsilon+f(x) - \\hat{f}(x))^2) \\\\\n",
    "    &= \\operatorname{\\mathbb{E}}(\\epsilon^2) + \\operatorname{\\mathbb{E}}((f(x)-\\hat{f}(x))^2) \\\\\n",
    "    &= \\sigma^2 + \\operatorname{\\mathbb{E}}(f(x)-\\hat{f}(x))^2 + Var(f(x)-\\hat{f}(x)) \\\\\n",
    "    &= \\sigma^2 + (Bias\\;\\hat{f}(x))^2 + Var(\\hat{f}(x))\n",
    "\\end{split}\n",
    "$$\n",
    "In equation three. Assume error term and $\\hat{f}$ is independent. Then the product of their expectation may go to zero since $\\operatorname{\\mathbb{E}}(\\epsilon_i) = 0$.\n",
    "\n",
    "- High Bias <==> Underfitting\n",
    "- High Variance <==> Overfitting\n",
    "- Large $\\sigma^2$ <==> Noisy data\n",
    "\n",
    "Another keypoint is that, most of the time reducing one will increase the other, and there is a tradeoff between bias and variance. Also this is done solely in the **linear regression** setting. For classification, there are no real agreement on what is the right or the most useful formalism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
