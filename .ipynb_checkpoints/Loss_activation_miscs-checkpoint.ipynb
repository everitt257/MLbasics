{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jacobian of cost function\n",
    "![The proof to this picture](http://om1hdizoc.bkt.clouddn.com/b12678222a016ce3ccb6f0bdc12166e2.png)\n",
    "\n",
    "The proof for the last statement is actually really simple.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla_{W^{[l]}}J(W,b) &= \\frac{\\partial J(w,b)}{\\partial w^{[l]}} \\\\\n",
    "&= \\frac{\\partial J(w,b)}{\\partial z^{[l]}}\\frac{\\partial z^{[l]}}{\\partial w^{[l]}}\n",
    "= \\frac{\\partial J(w,b)}{\\partial z^{[l]}}a^{[l-1]}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "## Common combinations of cost function and activation function at the final layer\n",
    "- Given loss function = MSE, final layer activation function = softmax (classifiying multiple things, output multiple neurons), the final layer gradient w.r.t to w is\n",
    "$$\\nabla_{z^{[l]}}L = (a^{[l]}-y)\\sigma\\prime(z^{[l]})$$\n",
    "$$\\text{Notice a, y, z are all vectors}$$\n",
    "$$\\text{This classification with the wrong loss function. it works but learns slowly}$$\n",
    "\n",
    "- Given loss function = cross-entropy, final layer activation function = sigmoid (determing the probability of somthing, output only one neuron), might need to normalize the actual label to 0~1.\n",
    "$$\\nabla_{z^{[l]}}L = (a^{[l]}-y)$$\n",
    "$$\\text{Notice a, y are scalar values}$$\n",
    "\n",
    "- Given loss function = cross-entropy, final layer activation function = softmax (classifiying multiple objects, output multiple neurons)\n",
    "$$\\nabla_{z^{[l]}}L = (a^{[l]}-y)$$\n",
    "$$\\text{Notice a, y are all vectors}$$\n",
    "$$\\text{This classification with the correct loss function.}$$\n",
    "> This is why we prefer softmax over MSE, it gets rid of the sigmoid term since it might saturate at very large or very low value.\n",
    "> The derivation for the partial derivative of softmax w.r.t z is [here](https://deepnotes.io/softmax-crossentropy). Notice the derivation seems complicated but it is closely related to logistical regression in the binary case.\n",
    "\n",
    "\n",
    "- Given loss function = MSE, final layer activation function = none (predicting continous signal), the final layer gradient w.r.t to w is\n",
    "$$\\nabla_{z^{[l]}}L = (a^{[l]}-y)$$\n",
    "$$\\text{Notice a, y are scalar values}$$\n",
    "\n",
    "### Other activation neuron\n",
    "See my other jupyter notbook about this part\n",
    "\n",
    "### Thoughts\n",
    "\n",
    ">- Question: How do you approach utilizing and researching machine learning techniques that are supported almost entirely empirically, as opposed to mathematically? Also in what situations have you noticed some of these techniques fail?\n",
    ">- Answer: You have to realize that our theoretical tools are very weak. Sometimes, we have good mathematical intuitions for why a particular technique should work. Sometimes our intuition ends up being wrong [...] The questions become: **how well does my method work on this particular problem, and how large is the set of problems on which it works well.**\n",
    "> - Question and answer with neural networks researcher *Yann LeCun*\n",
    "\n",
    "Indeed, we should not let the lack of a full theory stop us. We have powerful tool at hand, and can make a lot of of progress with these tools. Also, it's more important to explore boldly than it is to be rigorously correct in every step of your thinking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
